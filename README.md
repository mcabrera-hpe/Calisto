# Callisto - Multi-Agent Conversation Simulator

A local, Dockerized platform for building and testing AI agent-based PoCs with multi-agent conversation simulations.

[![Python](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-required-blue.svg)](https://www.docker.com/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## What is Callisto?

Callisto is a **standardized AI agent platform** designed for rapid PoC development. It enables you to:

- ğŸ¤– **Simulate multi-agent B2B conversations** (e.g., HPE sales vs Toyota procurement)
- ğŸ§™ **Generate scenarios dynamically** using LLM-powered wizards
- ğŸ‘¤ **Participate in conversations** or watch agents interact autonomously
- ğŸ“Š **Track sentiment in real-time** and analyze outcomes
- ğŸ”’ **Run 100% locally** with no cloud dependencies

**Perfect for:** AI/ML engineers building PoCs, testing conversation strategies, or experimenting with different LLM approaches.

---

## Quick Start

### Prerequisites

- **Docker** 20.10+ and **Docker Compose** 2.0+
- **16GB RAM** (recommended)
- **50GB free disk space**

### Setup (15 minutes)

```bash
# 1. Clone repository
git clone <repository-url>
cd callisto

# 2. Start services
docker-compose -f docker-compose.dev.yml up -d

# 3. Wait for services to be ready (~30-60 seconds)
docker-compose ps

# 4. Download LLM models (first time only, ~5GB)
docker-compose exec ollama ollama pull llama3.1
docker-compose exec ollama ollama pull nomic-embed-text

# 5. Initialize Weaviate
docker-compose exec app python scripts/init_weaviate.py

# 6. Load sample data
docker-compose exec app python scripts/ingest_documents.py \
  --company HPE --path /app/data/documents/hpe/

docker-compose exec app python scripts/ingest_documents.py \
  --company Toyota --path /app/data/documents/toyota/

# 7. Access the UI
open http://localhost:8501
```

**That's it!** You're ready to create your first agent scenario.

---

## Your First Simulation

### Scenario: HPE Selling Servers to Toyota

1. **Open UI:** http://localhost:8501

2. **Create Scenario:**
   - Select client: **Toyota**
   - Describe scenario: `"Negotiate server purchase contract"`
   - Click **Generate Agents**

3. **Review Suggested Agents:**
   - Agent 1: Sarah (HPE - Sales Engineer)
   - Agent 2: Yuki (Toyota - IT Procurement Manager)

4. **Run Simulation:**
   - Click **Run Simulation**
   - Watch agents negotiate in real-time
   - See sentiment chart update

5. **Review Results:**
   - Total turns taken
   - Final sentiment scores
   - Conversation outcome

**Pro Tip:** Check "I want to participate" to act as the HPE sales agent yourself!

---

## Features

### ğŸ¯ Core Capabilities

| Feature | Description |
|---------|-------------|
| **Dynamic Scenario Creation** | Describe your scenario in natural language â†’ LLM generates appropriate agents |
| **Multi-Tenant Knowledge** | Each company has isolated document storage with RAG retrieval |
| **Autonomous Conversations** | Watch agents negotiate without human intervention |
| **Human-in-the-Loop** | Participate as one of the agents in real-time |
| **Real-Time Sentiment** | Track emotional tone throughout conversation |
| **Full Reproducibility** | All configurations and prompts saved for analysis |

### ğŸ› ï¸ Tech Stack

- **LLM Server:** Ollama (llama3.1)
- **Vector Database:** Weaviate (multi-tenant)
- **RAG Pipeline:** LlamaIndex
- **UI:** Streamlit
- **Orchestration:** Docker Compose
- **Language:** Python 3.11
- **Dependencies:** Poetry

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Docker Compose Environment              â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Streamlit   â”‚  â”‚   Weaviate   â”‚           â”‚
â”‚  â”‚     UI       â”‚â†’â”‚  Vector DB   â”‚           â”‚
â”‚  â”‚              â”‚  â”‚  (Multi-tenant) â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚         â”‚                                       â”‚
â”‚         â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  â”‚    Ollama    â”‚                              â”‚
â”‚  â”‚  LLM Server  â”‚                              â”‚
â”‚  â”‚  â€¢ llama3.1  â”‚                              â”‚
â”‚  â”‚  â€¢ embeddingsâ”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data Flow:**
1. User describes scenario in Streamlit
2. LLM generates agent configurations
3. Agents query Weaviate (RAG) for company-specific knowledge
4. Agents generate responses via Ollama
5. Sentiment analyzed and displayed in real-time
6. Conversations saved to Weaviate + JSON exports

---

## Use Cases

### 1. Practice Client Conversations
Prepare for real client meetings by simulating tough negotiations.

**Example:** Practice responding to a client demanding 30% discount.

### 2. Compare LLM Performance
Test which model performs best for your use case.

**Example:** Run same scenario with llama3.1 vs mistral, compare outcomes.

### 3. Test Conversation Strategies
Validate different approaches before implementing.

**Example:** Aggressive pricing vs relationship-focused approach.

### 4. Multi-Party Simulations
Simulate complex deals with 4+ stakeholders.

**Example:** HPE sales + tech vs Client procurement + legal counsel.

---

## Project Structure

```
callisto/
â”œâ”€â”€ docker-compose.dev.yml      # Docker services definition
â”œâ”€â”€ Dockerfile                  # App container build
â”œâ”€â”€ pyproject.toml              # Python dependencies (Poetry)
â”œâ”€â”€ README.md                   # This file
â”‚
â”œâ”€â”€ src/                        # Application code
â”‚   â”œâ”€â”€ app.py                  # Streamlit UI
â”‚   â”œâ”€â”€ agents/                 # Agent logic
â”‚   â”œâ”€â”€ rag/                    # LlamaIndex RAG
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚
â”œâ”€â”€ scripts/                    # Setup & maintenance
â”‚   â”œâ”€â”€ init_weaviate.py       # Initialize database
â”‚   â””â”€â”€ ingest_documents.py    # Load documents
â”‚
â””â”€â”€ data/                       # Persistent data
    â”œâ”€â”€ conversations/          # Saved conversations (JSON)
    â””â”€â”€ documents/              # Input documents (PDF, DOCX)
        â”œâ”€â”€ hpe/
        â”œâ”€â”€ toyota/
        â””â”€â”€ microsoft/
```

---

## Adding Your Own Data

### 1. Add Documents

Place documents in `data/documents/<company_name>/`:

```bash
mkdir -p data/documents/acme
cp ~/contracts/*.pdf data/documents/acme/
```

### 2. Ingest Documents

```bash
docker-compose exec app python scripts/ingest_documents.py \
  --company "Acme Corp" \
  --path /app/data/documents/acme/
```

The script will:
- Convert company name to tenant ID (`acme_corp`)
- Extract text from PDFs/DOCX
- Chunk content into 512-token segments
- Generate embeddings via Ollama
- Store in Weaviate with tenant isolation

### 3. Use in Scenarios

Select "Acme Corp" in the client dropdown, and agents will have access to those documents.

---

## Advanced Usage

### Human Participation Mode

Act as one of the agents yourself:

1. Check **"I want to participate"** in scenario wizard
2. System generates: You + AI client agent
3. Chat with AI client in real-time
4. System tracks sentiment of client responses

### Multi-Agent Scenarios

Create complex simulations with 4+ agents:

**Example Prompt:**
```
"Enterprise deal with HPE and Microsoft involving 
technical architects and legal counsel on both sides"
```

System generates:
- HPE Account Manager
- HPE Technical Architect
- Microsoft Procurement Officer
- Microsoft Legal Counsel

### Regenerate Agents

Not satisfied with suggested agents?
1. Click **"Regenerate Agents"**
2. System creates new configuration with different roles/names
3. Repeat until satisfied

---

## Configuration

### Environment Variables

Set in `docker-compose.dev.yml`:

```yaml
environment:
  - WEAVIATE_URL=http://weaviate:8080
  - OLLAMA_URL=http://ollama:11434
  - DEFAULT_MODEL=llama3.1
  - MAX_TURNS=30
```

### Streamlit Customization

Edit `.streamlit/config.toml`:

```toml
[theme]
primaryColor = "#FF4B4B"
backgroundColor = "#0E1117"
secondaryBackgroundColor = "#262730"
textColor = "#FAFAFA"
font = "sans serif"
```

---

## Troubleshooting

### Services Won't Start

```bash
# Check status
docker-compose ps

# View logs
docker-compose logs -f app
docker-compose logs -f ollama
docker-compose logs -f weaviate

# Restart services
docker-compose restart
```

### Ollama Model Not Found

```bash
# List downloaded models
docker-compose exec ollama ollama list

# Pull missing model
docker-compose exec ollama ollama pull llama3.1
```

### Out of Memory

Increase Docker memory limit:
- **Docker Desktop:** Settings â†’ Resources â†’ Memory (set to 12GB+)

### Slow Responses

- Use smaller model: `ollama pull llama3.1:8b`
- Reduce context window in agent prompts
- Close other applications

### Weaviate Connection Error

```bash
# Check Weaviate health
curl http://localhost:8080/v1/.well-known/ready

# Restart Weaviate
docker-compose restart weaviate
```

---

## Development

### Hot-Reload

Code changes in `src/` automatically reload in Streamlit:

```bash
# Edit src/app.py
# Save file
# Streamlit detects change and reloads (5-10 seconds)
```

### Add Python Dependencies

```bash
# Add package
poetry add <package-name>

# Update lock file
poetry lock

# Rebuild container
docker-compose build app
docker-compose up -d app
```

### Run Tests (Future)

```bash
docker-compose exec app pytest
```

---

## Performance

### Expected Response Times (Alpha)

| Operation | Target | Actual (Laptop) |
|-----------|--------|-----------------|
| Scenario generation | < 10s | ~8s |
| Agent response (no RAG) | < 5s | ~3s |
| RAG retrieval | < 3s | ~2s |
| Page load | < 2s | ~1s |

**Hardware:** MacBook Pro M2, 16GB RAM

### Resource Usage

- **Idle:** ~2GB RAM
- **Active conversation:** ~6GB RAM
- **Disk:** ~15GB (models + data)

---

## Roadmap

### âœ… Alpha (Current)
- Dynamic scenario creation
- Multi-agent conversations
- Human participation mode
- Real-time sentiment tracking
- Conversation persistence

### ğŸ”„ Phase 2 (Next)
- Scenario template library
- Agent role presets
- Advanced sentiment (emotions)
- Export to PDF/Markdown

### ğŸ“‹ Phase 3 (Future)
- Phoenix observability
- MLflow experiment tracking
- RAGAS evaluation metrics
- A/B testing framework

### ğŸš€ Phase 4+ (Backlog)
- Multi-user support
- Team collaboration
- Production deployment
- API for integrations

See [Implementation Plan](docs/Implementation%20Plan%20-%20Callisto.md) for details.

---

## FAQ

**Q: Does this work offline?**  
A: Yes! All processing is local. No internet required after initial setup.

**Q: Can I use different LLMs?**  
A: Yes. Pull any Ollama-supported model (`ollama pull mistral`) and update config.

**Q: How accurate is sentiment analysis?**  
A: Uses distilBERT (~80% accuracy). Good for trends, not perfect.

**Q: Can agents access the internet?**  
A: No. Agents only access uploaded documents via RAG.

**Q: How do I delete old conversations?**  
A: Delete from `data/conversations/` folder. Weaviate cleanup coming in Phase 2.

**Q: Can I run this in production?**  
A: Not recommended. Alpha is for experimentation. See Phase 5 for production hardening.

---

## Documentation

Comprehensive documentation in `docs/`:

- **[Business Requirements](docs/Business%20Requirements%20Document%20-%20Callisto.md)** - Why this exists, use cases, goals
- **[Technical Requirements](docs/Technical%20Requirements%20Document%20-%20Callisto.md)** - Architecture, APIs, data models
- **[Implementation Plan](docs/Implementation%20Plan%20-%20Callisto.md)** - Development roadmap, phases, timelines

---

## Contributing

This is a personal project for rapid PoC development. Not currently accepting external contributions.

If you find this useful and want to adapt it:
1. Fork the repository
2. Customize for your use case
3. Share your learnings!

---

## License

MIT License - See [LICENSE](LICENSE) file for details.

---

## Acknowledgments

Built with:
- [Ollama](https://ollama.ai/) - Local LLM inference
- [Weaviate](https://weaviate.io/) - Vector database
- [LlamaIndex](https://www.llamaindex.ai/) - RAG framework
- [Streamlit](https://streamlit.io/) - Web UI
- [HuggingFace Transformers](https://huggingface.co/transformers/) - Sentiment analysis

---

## Contact

**Author:** Senior AI/ML Engineer  
**Project:** Callisto  
**Purpose:** Standardized local AI agent platform for PoC development

For questions or feedback, open an issue in the repository.

---

## Version History

- **v0.1.0 (Alpha)** - January 30, 2026
  - Initial release
  - Core agent functionality
  - LLM-powered scenario generation
  - Multi-tenant RAG
  - Human-in-the-loop mode
  - Real-time sentiment tracking

---

**Happy Simulating! ğŸ¤–**
